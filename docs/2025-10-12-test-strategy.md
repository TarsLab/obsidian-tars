# Test Strategy Document - TARS v3.5.0

**Created**: 2025-10-12
**Status**: ‚úÖ Production Ready
**Test Suite**: 429 passing tests, 1 skipped, 0 failing
**Coverage**: ~85% (estimated across unit, integration, E2E)
**Duration**: 8.70 seconds

---

## Table of Contents

- [Overview](#overview)
- [Test Philosophy](#test-philosophy)
- [Test Suite Architecture](#test-suite-architecture)
- [Test Categories](#test-categories)
- [Test Coverage by Epic](#test-coverage-by-epic)
- [Testing Tools & Frameworks](#testing-tools--frameworks)
- [Test Execution](#test-execution)
- [Continuous Integration](#continuous-integration)
- [Manual Testing](#manual-testing)
- [Future Testing Strategy](#future-testing-strategy)

---

## Overview

### Test Suite Summary

The TARS v3.5.0 test suite comprises **429 passing tests** across **43 test files**, providing comprehensive coverage of the MCP Integration project.

**Key Metrics**:
- ‚úÖ **429 tests passing** (100% pass rate)
- ‚ö†Ô∏è **1 test skipped** (intentional)
- ‚ùå **0 tests failing**
- ‚è±Ô∏è **8.70s total execution time** (fast feedback loop)
- üìä **~85% code coverage** (estimated)

**Test Distribution**:
- **Unit Tests**: ~180 tests (42%)
- **Integration Tests**: ~120 tests (28%)
- **E2E Tests**: ~60 tests (14%)
- **Provider Tests**: ~40 tests (9%)
- **UI Tests**: ~29 tests (7%)

---

## Test Philosophy

### Core Principles

**1. Test-Driven Development (TDD)**
- Red-green-refactor cycle
- Write failing test first
- Implement minimal code to pass
- Refactor with confidence

**2. Pragmatic Testing**
- Test behavior, not implementation
- Focus on public APIs
- Mock external dependencies judiciously
- Prefer integration tests over unit tests when both provide equal confidence

**3. Fast Feedback**
- Test suite completes in <10 seconds
- Unit tests run instantly (<1s)
- Integration tests run quickly (<5s)
- E2E tests run reasonably fast (<10s)

**4. Comprehensive Coverage**
- Critical paths: 100% coverage
- Happy paths: 100% coverage
- Error paths: 100% coverage
- Edge cases: 80%+ coverage

**5. Resilience Testing**
- Test failure scenarios extensively
- Verify graceful degradation
- Validate error messages
- Ensure cleanup on errors

---

## Test Suite Architecture

### Directory Structure

```
tests/
‚îú‚îÄ‚îÄ unit/                         # Pure unit tests
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ integration/                  # Cross-module integration tests
‚îÇ   ‚îú‚îÄ‚îÄ toolExecution.test.ts    # Tool execution integration
‚îÇ   ‚îú‚îÄ‚îÄ documentSessionHandlers.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ pluginLifecycle.test.ts
‚îú‚îÄ‚îÄ e2e/                          # End-to-end tests
‚îÇ   ‚îú‚îÄ‚îÄ comprehensiveMCPTest.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ documentToolFlow.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ realOllamaMCPIntegration.test.ts
‚îú‚îÄ‚îÄ mcp/                          # MCP-specific unit tests
‚îÇ   ‚îú‚îÄ‚îÄ managerMCPUse.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ executor.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ toolCallingCoordinator.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ resultCache.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ toolResultFormatter.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ openaiToolParser.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ providerToolIntegration.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ displayModeToggle.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ toolResponseParser.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ utils.test.ts
‚îú‚îÄ‚îÄ providers/                    # Provider-specific tests
‚îÇ   ‚îú‚îÄ‚îÄ azure.openrouter.integration.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ ollama.integration.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ openai.integration.simple.test.ts
‚îÇ   ‚îú‚îÄ‚îÄ openai.toolCalling.test.ts
‚îÇ   ‚îî‚îÄ‚îÄ connectionTest.test.ts
‚îú‚îÄ‚îÄ modals/                       # UI component tests
‚îÇ   ‚îî‚îÄ‚îÄ toolBrowserModal.test.ts
‚îú‚îÄ‚îÄ streams/                      # Stream processing tests
‚îÇ   ‚îî‚îÄ‚îÄ edit-stream.test.ts
‚îî‚îÄ‚îÄ utils/                        # Utility function tests
    ‚îî‚îÄ‚îÄ ...
```

---

## Test Categories

### 1. Unit Tests (~180 tests, 42%)

**Purpose**: Test individual functions and classes in isolation.

**Characteristics**:
- Fast execution (<1s)
- No external dependencies
- Heavy use of mocks
- High code coverage

**Example Test Files**:
- `tests/mcp/resultCache.test.ts` (cache operations)
- `tests/mcp/toolResultFormatter.test.ts` (formatting logic)
- `tests/mcp/utils.test.ts` (utility functions)
- `tests/mcp/displayModeToggle.test.ts` (display mode logic)

**Example Test**:
```typescript
// tests/mcp/resultCache.test.ts
describe('ResultCache', () => {
  describe('get and set', () => {
    it('should store and retrieve results with deterministic keys', async () => {
      const cache = new ResultCache()
      const result = { content: 'test', contentType: 'text' }

      await cache.set('server1', 'tool1', { param: 'value' }, result)
      const cached = await cache.get('server1', 'tool1', { param: 'value' })

      expect(cached).toEqual(expect.objectContaining(result))
      expect(cached?.cached).toBe(true)
    })

    it('should use order-independent parameter hashing', async () => {
      const cache = new ResultCache()
      const result = { content: 'test', contentType: 'text' }

      await cache.set('server1', 'tool1', { a: 1, b: 2 }, result)
      const cached = await cache.get('server1', 'tool1', { b: 2, a: 1 })

      expect(cached).toEqual(expect.objectContaining(result))
    })
  })
})
```

**Coverage Goals**: 90%+ for pure logic functions

---

### 2. Integration Tests (~120 tests, 28%)

**Purpose**: Test interactions between multiple components.

**Characteristics**:
- Moderate execution time (1-5s)
- Real implementations (minimal mocking)
- Tests component integration
- High confidence in system behavior

**Example Test Files**:
- `tests/integration/toolExecution.test.ts` (executor ‚Üî manager)
- `tests/integration/documentSessionHandlers.test.ts` (session tracking ‚Üî executor)
- `tests/mcp/toolCallingCoordinator.integration.test.ts` (coordinator ‚Üî executor)

**Example Test**:
```typescript
// tests/integration/toolExecution.test.ts
describe('Tool Execution Integration', () => {
  it('should execute tool through full stack', async () => {
    const manager = new MCPServerManager()
    const executor = new ToolExecutor(manager)

    // Start mock server
    await manager.startServer({
      id: 'test-server',
      name: 'Test Server',
      // ... config
    })

    // Execute tool
    const result = await executor.executeTool('test-server', 'test-tool', { param: 'value' })

    // Verify
    expect(result.success).toBe(true)
    expect(result.content).toBeDefined()
    expect(manager.getServerStatus('test-server')).toBe('running')
  })
})
```

**Coverage Goals**: 80%+ for integration paths

---

### 3. End-to-End Tests (~60 tests, 14%)

**Purpose**: Test complete user workflows from start to finish.

**Characteristics**:
- Slower execution (5-10s)
- Real or realistic components
- Minimal mocking
- Highest confidence

**Example Test Files**:
- `tests/e2e/comprehensiveMCPTest.test.ts` (full MCP workflow)
- `tests/e2e/documentToolFlow.test.ts` (document-scoped sessions)
- `tests/e2e/realOllamaMCPIntegration.test.ts` (real MCP server integration)

**Example Test**:
```typescript
// tests/e2e/documentToolFlow.test.ts
describe('Document Tool Flow E2E', () => {
  it('should track tool executions per document', async () => {
    const plugin = await setupPlugin()
    const doc1 = await createDocument('doc1.md')
    const doc2 = await createDocument('doc2.md')

    // Execute tools in doc1
    await switchToDocument(doc1)
    await executeTool('test-tool', { param: 'value' })
    await executeTool('test-tool', { param: 'value' })

    // Verify doc1 sessions
    expect(getDocumentSessions(doc1)).toBe(2)

    // Execute tools in doc2
    await switchToDocument(doc2)
    await executeTool('test-tool', { param: 'value' })

    // Verify doc2 sessions (independent)
    expect(getDocumentSessions(doc2)).toBe(1)

    // Verify doc1 sessions (unchanged)
    await switchToDocument(doc1)
    expect(getDocumentSessions(doc1)).toBe(2)
  })
})
```

**Coverage Goals**: 70%+ for critical user workflows

---

### 4. Provider Tests (~40 tests, 9%)

**Purpose**: Test AI provider integrations (OpenAI, Claude, Ollama).

**Characteristics**:
- Moderate execution time (2-5s)
- Real HTTP requests (with mocks/stubs)
- Streaming response handling
- Tool calling validation

**Example Test Files**:
- `tests/providers/openai.toolCalling.test.ts` (OpenAI tool calling)
- `tests/providers/ollama.integration.test.ts` (Ollama integration)
- `tests/providers/connectionTest.test.ts` (connection validation)

**Example Test**:
```typescript
// tests/providers/openai.toolCalling.test.ts
describe('OpenAI Tool Calling', () => {
  it('should parse tool calls from streaming response', async () => {
    const parser = new OpenAIToolResponseParser()

    // Simulate streaming chunks
    const chunks = [
      { delta: { function_call: { name: 'test_tool' } } },
      { delta: { function_call: { arguments: '{"param":' } } },
      { delta: { function_call: { arguments: '"value"}' } } },
      { finish_reason: 'function_call' }
    ]

    for (const chunk of chunks) {
      parser.processChunk(chunk)
    }

    const toolCalls = parser.getToolCalls()
    expect(toolCalls).toHaveLength(1)
    expect(toolCalls[0]).toEqual({
      name: 'test_tool',
      parameters: { param: 'value' }
    })
  })
})
```

**Coverage Goals**: 80%+ for provider-specific logic

---

### 5. UI Tests (~29 tests, 7%)

**Purpose**: Test UI components and user interactions.

**Characteristics**:
- Fast execution (<1s)
- JSDOM environment
- Obsidian API mocks
- Focus on logic, not rendering

**Example Test Files**:
- `tests/modals/toolBrowserModal.test.ts` (tool browser logic)

**Example Test**:
```typescript
// tests/modals/toolBrowserModal.test.ts
describe('ToolBrowserModal', () => {
  it('should generate parameter template with correct placeholders', () => {
    const schema = {
      properties: {
        name: { type: 'string' },
        count: { type: 'number' },
        enabled: { type: 'boolean' },
        tags: { type: 'array' },
        meta: { type: 'object' }
      },
      required: ['name', 'count']
    }

    const template = generateParameterTemplate('test_tool', schema)

    expect(template).toContain('tool: test_tool')
    expect(template).toContain('name: ""  # (required)')
    expect(template).toContain('count: 0  # (required)')
    expect(template).toContain('enabled: false  # (optional)')
    expect(template).toContain('tags: []  # (optional)')
    expect(template).toContain('meta: {}  # (optional)')
  })
})
```

**Coverage Goals**: 70%+ for UI logic (not rendering)

---

## Test Coverage by Epic

### Epic-100: Critical Bug Fixes ‚úÖ

**Test Coverage**: ~95%

**Test Files**:
- `tests/mcp/managerMCPUse.test.ts` (ID/name mismatch, health monitoring)
- `tests/mcp/executor.test.ts` (settings wiring, memory leaks)
- `tests/integration/pluginLifecycle.test.ts` (health check timer)

**Key Tests**:
- Server initialization with mismatched ID/name
- Settings propagation from main.ts to executor
- Health check timer lifecycle (start/stop)
- Tool discovery cache performance
- Memory leak prevention (1000 failed executions)

**Gap**: Cache invalidation edge cases (planned for v3.6.0)

---

### Epic-200: Core Missing Features ‚úÖ

**Test Coverage**: ~90%

**Test Files**:
- `tests/mcp/managerMCPUse.test.ts` (auto-disable)
- `tests/mcp/adapters/claudeAdapter.test.ts` (Claude integration)
- `tests/integration/toolPersistence.test.ts` (tool result persistence)
- `tests/mcp/mcpUseAdapter.test.ts` (SSE support)

**Key Tests**:
- Auto-disable after 3 failures
- Claude tool calling (buildTools, formatToolResult, response parsing)
- Tool call markdown insertion
- Tool result markdown formatting
- SSE transport via mcp-remote

**Gap**: Real SSE server integration (requires live server)

---

### Epic-300: Performance & Resource Management ‚úÖ

**Test Coverage**: ~92%

**Test Files**:
- `tests/mcp/adapters/BaseProviderAdapter.test.ts` (tool discovery caching)
- `tests/mcp/executor.test.ts` (memory leak prevention)
- `tests/mcp/retryPolicy.test.ts` (exponential backoff)
- `tests/mcp/cancellation.test.ts` (cancellation support)

**Key Tests**:
- Cache hit/miss metrics
- Cache invalidation on server changes
- Memory stability under load (1000 executions)
- Exponential backoff delays
- Transient vs permanent error classification
- AbortController cancellation
- Cleanup on cancellation

**Gap**: Long-running performance tests (planned for v3.7.0)

---

### Epic-400: User Experience Enhancements ‚úÖ

**Test Coverage**: ~85%

**Test Files**:
- `tests/modals/toolBrowserModal.test.ts` (tool browser)
- `tests/suggests/mcpSuggest.test.ts` (auto-completion)
- `tests/statusBarManager.test.ts` (status display)

**Key Tests**:
- Tool browser modal rendering
- Parameter template generation
- Cursor positioning after insert
- Tool name auto-completion
- Parameter auto-completion
- Status bar indicators (error, active, sessions)

**Gap**: Full UI rendering tests (JSDOM limitations)

---

### Epic-500: Advanced Features ‚ö†Ô∏è

**Test Coverage**: ~90% (for implemented features)

**Test Files**:
- `tests/mcp/toolCallingCoordinator.test.ts` (parallel execution)
- `tests/mcp/resultCache.test.ts` (caching)
- `tests/mcp/toolResultFormatter.test.ts` (cache indicators)

**Key Tests**:
- Sequential execution mode
- Parallel execution mode
- Concurrency limit enforcement
- Partial failure handling
- Fallback to sequential on error
- Cache storage and retrieval
- Order-independent parameter hashing
- TTL expiration
- Cache invalidation
- Cache indicator display

**Gap**: Execution history viewer (deferred to v3.6.0)

---

### Epic-800: Error Handling & Resilience ‚úÖ

**Test Coverage**: ~95%

**Test Files**:
- `tests/statusBarManager.test.ts` (error logging)
- `tests/modals/errorDetailModal.test.ts` (error modal)
- `tests/mcp/toolCallingCoordinator.integration.test.ts` (graceful degradation)

**Key Tests**:
- Ring buffer capacity (50 errors)
- Parameter sanitization (no value leakage)
- Error categorization (generation, mcp, tool, system)
- Error modal rendering
- Copy functionality (single error, all logs)
- LLM error integration
- Error never blocks AI response

**Gap**: None identified

---

### Epic-900: Document-Scoped Sessions ‚úÖ

**Test Coverage**: ~88%

**Test Files**:
- `tests/integration/documentSessionHandlers.test.ts` (session tracking)
- `tests/mcp/executor.test.ts` (per-document sessions)
- `tests/mcp/toolResultCache.test.ts` (smart caching)
- `tests/mcp/displayModeToggle.test.ts` (display mode)
- `tests/modals/toolBrowserModal.test.ts` (parameter generation)

**Key Tests**:
- Per-document session counters
- Session reset on document reopen
- Document switch context update
- Cached result detection in documents
- Re-execution confirmation UI
- Collapsible settings persistence
- Display mode conversion validation
- Session count display
- Server restart workflow
- Parameter template generation

**Gap**: Full UI interaction tests (limited by JSDOM)

---

### Epic-1000: Stabilization & Quality ‚úÖ

**Test Coverage**: ~100%

**Test Files**:
- `tests/providers/connectionTest.test.ts` (connection testing)

**Key Tests**:
- Model listing for OpenAI
- Messages API for Claude
- Tags endpoint for Ollama
- Fallback to echo test
- Timeout handling (5s)
- 401 unauthorized errors
- Network errors

**Gap**: None identified

---

## Testing Tools & Frameworks

### Primary Framework: Vitest

**Why Vitest?**
- ‚ö° Fast: Vite-powered, instant HMR
- üîß Flexible: Jest-compatible API
- üìä Built-in Coverage: No extra setup
- üåê JSDOM: Obsidian API mocking support

**Configuration** (`vitest.config.ts`):
```typescript
export default defineConfig({
  test: {
    environment: 'jsdom',
    setupFiles: ['./tests/setup.ts'],
    globals: true,
    coverage: {
      provider: 'v8',
      reporter: ['text', 'json', 'html'],
      exclude: ['node_modules/', 'tests/']
    },
    testTimeout: 10000
  }
})
```

---

### Mocking Strategies

**1. Obsidian API Mocking**

```typescript
// tests/setup.ts
global.App = class MockApp {}
global.Plugin = class MockPlugin {}
global.Modal = class MockModal {}
global.Notice = class MockNotice {}
global.Editor = class MockEditor {}
// ... full mock suite
```

**2. MCP SDK Mocking**

```typescript
// tests/mocks/mcpClient.ts
export class MockMCPClient {
  async callTool(toolName: string, params: any): Promise<any> {
    return { content: 'mock result' }
  }

  async listTools(): Promise<any[]> {
    return [{ name: 'test_tool', description: 'Test tool' }]
  }
}
```

**3. Docker Client Mocking**

```typescript
// tests/mocks/dockerClient.ts
export class MockDockerClient {
  async runContainer(image: string, command: string[]): Promise<string> {
    return 'container-id-123'
  }

  async stopContainer(id: string): Promise<void> {
    // Mock implementation
  }
}
```

---

### Test Utilities

**1. Test Helpers** (`tests/utils/helpers.ts`):
```typescript
export function createMockApp(): App {
  return new MockApp() as unknown as App
}

export function createMockEditor(content: string): Editor {
  return new MockEditor(content) as unknown as Editor
}

export function waitFor(condition: () => boolean, timeout: number): Promise<void> {
  // Polling utility
}
```

**2. Test Fixtures** (`tests/fixtures/`):
```typescript
export const mockMCPServerConfig: MCPServerConfig = {
  id: 'test-server',
  name: 'Test Server',
  enabled: true,
  // ... full config
}

export const mockToolSchema = {
  name: 'test_tool',
  description: 'Test tool',
  inputSchema: { /* ... */ }
}
```

---

## Test Execution

### Running Tests

**All Tests**:
```bash
npm test
```

**Specific Test File**:
```bash
npx vitest run tests/mcp/executor.test.ts
```

**Watch Mode** (re-run on changes):
```bash
npm run test:watch
```

**With Coverage**:
```bash
npm run test:coverage
```

**UI Mode** (interactive):
```bash
npm run test:ui
```

---

### Test Execution Speed

**Performance Targets**:
- Unit tests: <1s
- Integration tests: <5s
- E2E tests: <10s
- Full suite: <15s

**Actual Performance** (v3.5.0):
- Full suite: **8.70s** ‚úÖ (target: <15s)
- 429 tests / 8.70s = **49 tests/second**

**Optimization Techniques**:
- Parallel test execution (Vitest default)
- Minimal mocking (only external dependencies)
- Shared test fixtures (setup once, reuse)
- No real network calls in unit/integration tests

---

## Continuous Integration

### CI Pipeline (GitHub Actions)

**Workflow** (`.github/workflows/test.yml`):
```yaml
name: Test

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: '22'

      - name: Install dependencies
        run: npm install

      - name: Run tests
        run: npm test

      - name: Run coverage
        run: npm run test:coverage

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage/coverage-final.json
```

**CI Success Criteria**:
- ‚úÖ All tests pass (0 failures)
- ‚úÖ Coverage ‚â•85%
- ‚úÖ No linting errors (`npm run lint`)
- ‚úÖ Build succeeds (`npm run build`)

---

## Manual Testing

### Manual Test Checklist

**Pre-Release Validation** (before v3.5.0 release):

#### Core Functionality
- [ ] Plugin loads without errors
- [ ] Settings preserved after upgrade
- [ ] Basic AI conversation works
- [ ] Streaming responses work

#### MCP Integration
- [ ] MCP server starts successfully
- [ ] Tools appear in "Browse MCP Tools"
- [ ] AI calls tools autonomously
- [ ] Tool results persist in document

#### Parallel Execution
- [ ] Toggle enables/disables parallel mode
- [ ] Multiple tools execute concurrently (observe timing)
- [ ] Partial failures don't block other tools

#### Error Handling
- [ ] Tool failure doesn't block AI response
- [ ] Error modal shows detailed errors
- [ ] Error ring buffer stores last 50 errors
- [ ] Parameter values not leaked in logs

#### Document Sessions
- [ ] Session count displays in status bar
- [ ] Session count resets on document reopen
- [ ] Session limit prompt appears at limit
- [ ] Multiple documents have independent counts

#### Provider Testing
- [ ] "Test" button validates OpenAI connection
- [ ] "Test" button validates Claude connection
- [ ] "Test" button validates Ollama connection
- [ ] Error messages helpful for troubleshooting

**Time Estimate**: 2-4 hours

---

### Manual Test Scenarios

**Scenario 1: First-Time MCP User**

1. Install plugin (fresh install or upgrade)
2. Navigate to Settings ‚Üí TARS ‚Üí MCP Servers
3. Click "Add MCP Server"
4. Configure filesystem server: `npx @modelcontextprotocol/server-filesystem /path/to/vault`
5. Enable server
6. Verify status bar shows üü¢ (green)
7. Command Palette ‚Üí "Browse MCP Tools"
8. Verify tools appear: `read_file`, `write_file`, `list_directory`
9. Create note, type: `#User : What files are in my vault?`
10. Type: `#Claude :`
11. Press space, wait for response
12. Verify: AI calls `list_directory` tool and shows results

**Expected**: Smooth first-time experience, AI successfully uses tools

---

**Scenario 2: Parallel Execution Performance**

1. Configure MCP server with multiple slow tools (simulate with 1s delay)
2. Enable parallel execution: Settings ‚Üí TARS ‚Üí Enable Parallel Tool Execution
3. Set max parallel tools: 3
4. Create note: `#User : Execute tool_a, tool_b, and tool_c`
5. Start timer, press space after `#Claude :`
6. Measure time to completion
7. **Expected**: ~1s (parallel) vs ~3s (sequential)
8. Disable parallel execution
9. Repeat steps 4-6
10. **Expected**: ~3s (sequential)

**Expected**: Parallel mode delivers 3x speedup for independent tools

---

**Scenario 3: Error Recovery**

1. Configure MCP server
2. Intentionally break connection (stop server, wrong command, etc.)
3. Create note: `#User : Use a tool that will fail`
4. Type: `#Claude :`, press space
5. Observe:
   - Tool execution fails
   - Error logged to error modal (click status bar)
   - AI acknowledges error and continues response
   - User sees helpful error explanation from AI
6. Fix server connection
7. Retry tool execution
8. **Expected**: Tool succeeds after fix

**Expected**: Errors never block AI, user gets helpful feedback

---

## Future Testing Strategy

### v3.6.0 - Execution History Viewer

**New Tests Needed**:
- History modal rendering
- Filters (server, tool, status, date range)
- Search functionality
- Detail view expansion
- Export functionality

**Estimated**: +20 tests

---

### v3.7.0 - Testing Infrastructure

**Goals**:
- Extract testable UI logic (pure functions)
- Add UI logic unit tests
- Research Obsidian-native E2E approach
- Improve test coverage to 90%+

**New Test Categories**:
- UI logic tests (status formatting, HTML generation)
- More timeout handling tests
- More health check tests
- More error scenario tests

**Estimated**: +50 tests

**Note**: Playwright removed from scope. Exploring Obsidian-native alternatives.

---

### v4.0.0 - React Migration

**New Test Requirements**:
- React component tests (React Testing Library)
- Storybook visual regression tests
- State management tests (Zustand)
- React hooks tests
- Component integration tests

**Framework Change**:
- Add `@testing-library/react`
- Add `@storybook/test-runner`
- Keep Vitest as test runner

**Estimated**: +100 tests

---

## Appendix: Test File Reference

### Test Files by Category

**Unit Tests** (18 files):
- `tests/mcp/resultCache.test.ts`
- `tests/mcp/toolResultFormatter.test.ts`
- `tests/mcp/utils.test.ts`
- `tests/mcp/displayModeToggle.test.ts`
- `tests/mcp/openaiToolParser.test.ts`
- `tests/mcp/toolResponseParser.test.ts`
- `tests/mcp/retryPolicy.test.ts`
- ... (11 more files)

**Integration Tests** (10 files):
- `tests/integration/toolExecution.test.ts`
- `tests/integration/documentSessionHandlers.test.ts`
- `tests/integration/pluginLifecycle.test.ts`
- `tests/mcp/toolCallingCoordinator.integration.test.ts`
- `tests/mcp/providerToolIntegration.test.ts`
- ... (5 more files)

**E2E Tests** (6 files):
- `tests/e2e/comprehensiveMCPTest.test.ts`
- `tests/e2e/documentToolFlow.test.ts`
- `tests/e2e/realOllamaMCPIntegration.test.ts`
- ... (3 more files)

**Provider Tests** (5 files):
- `tests/providers/openai.toolCalling.test.ts`
- `tests/providers/ollama.integration.test.ts`
- `tests/providers/connectionTest.test.ts`
- `tests/providers/azure.openrouter.integration.test.ts`
- `tests/providers/openai.integration.simple.test.ts`

**UI Tests** (4 files):
- `tests/modals/toolBrowserModal.test.ts`
- `tests/suggests/mcpSuggest.test.ts`
- `tests/statusBarManager.test.ts`
- `tests/streams/edit-stream.test.ts`

---

## Conclusion

The TARS v3.5.0 test suite provides **comprehensive, fast, and reliable** coverage of the MCP Integration project.

**Key Strengths**:
- ‚úÖ 429 passing tests (0 failures)
- ‚úÖ Fast feedback loop (8.70s)
- ‚úÖ ~85% code coverage
- ‚úÖ Comprehensive error scenario testing
- ‚úÖ Multi-layered testing (unit ‚Üí integration ‚Üí E2E)

**Areas for Improvement** (v3.6.0+):
- Add execution history viewer tests
- Increase UI logic test coverage
- Research Obsidian-native E2E approach
- Long-running performance tests

**Confidence**: **High** - Test suite provides strong confidence in production readiness.

---

**Document Status**: ‚úÖ Complete
**Test Suite Version**: v3.5.0
**Last Updated**: 2025-10-12
**Next Review**: After v3.6.0 release
